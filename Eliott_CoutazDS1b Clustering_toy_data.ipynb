{"cells":[{"cell_type":"code","execution_count":null,"metadata":{"id":"y-9ezimxhTur"},"outputs":[],"source":["!pip install scprep graphtools louvain"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"EzZTA--MhTuu"},"outputs":[],"source":["import time\n","\n","import numpy as np\n","import pandas as pd\n","import matplotlib.pyplot as plt\n","%matplotlib inline\n","plt.rc('font', size=14)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"4PhO9XauhTuu"},"outputs":[],"source":["import sklearn\n","import sklearn.cluster\n","import sklearn.datasets\n","\n","import scprep\n","import louvain\n","import graphtools as gt"]},{"cell_type":"markdown","metadata":{"id":"BsFxz2rHhTuv"},"source":["# Comparison of clustering algorithms on toy data\n","\n","Here we're going to compare three clustering algorithms on toy data. The code for this exercise is adapted from: https://scikit-learn.org/stable/auto_examples/cluster/plot_cluster_comparison.html"]},{"cell_type":"markdown","metadata":{"id":"INycrwiIhTuw"},"source":["### Generating data\n","\n","Here we're going to work with 6 datasets all in $\\mathbb{R}^2$\n","\n","1. **Circles** - two circles, one circumscribed by the other\n","2. **Moons** - Two interleaving half circles\n","3. **Varied variance blobs** - These blobs each have different variances\n","4. **Anisotropically distributed blobs** - these blobs have unequal widths and lengths\n","5. **Regular blobs** - Just three regular blobs\n","6. **Uniformly sampled square** - Just a single square\n","\n","Because we're generating these from scratch, we get to change some parameters of their distributions. Generally, we can change:\n","1. `noise` - the amount of Gaussian noise added to each point\n","2. `n_samples` - the number of points generated\n","3. `factor` / `cluster_std` - some parameters affecting shape"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"e3o0JTYFhTux"},"outputs":[],"source":["np.random.seed(0)\n","# ============\n","# Generate datasets. We choose the size big enough to see the scalability\n","# of the algorithms, but not too big to avoid too long running times\n","# ============\n","n_samples = 1500\n","\n","# Circles\n","noisy_circles = sklearn.datasets.make_circles(\n","    n_samples=n_samples,\n","    # Scale factor between inner and outer circle\n","    factor=.5,\n","    # Gaussian noise added to each point\n","    noise=.05)\n","\n","# Moons\n","noisy_moons = sklearn.datasets.make_moons(n_samples=n_samples,\n","                                          noise=.05)\n","\n","# Blobs\n","blobs = sklearn.datasets.make_blobs(n_samples=n_samples, random_state=8, cluster_std=1)\n","\n","# Uniform square\n","no_structure = (np.random.uniform(size=(n_samples, 2)), None)\n","\n","# Anisotropically distributed data\n","random_state = 170\n","X, y = sklearn.datasets.make_blobs(n_samples=n_samples, random_state=random_state, cluster_std=1)\n","# Changes how x1, x2 coordinates are shifted\n","transformation = [[0.6, -0.6], [-0.4, 0.8]]\n","X_aniso = np.dot(X, transformation)\n","aniso = (X_aniso, y)\n","\n","# blobs with varied variances\n","varied = sklearn.datasets.make_blobs(n_samples=n_samples,\n","                                     cluster_std=[1.0, 2.5, 0.5],\n","                                     random_state=random_state)\n","\n","# ============\n","# Associate each dataset with the correct # of clusters\n","# ============\n","\n","default_base = {'n_clusters': 3}\n","\n","generated_datasets = [\n","    (noisy_circles, {'n_clusters': 2}),\n","    (noisy_moons, {'n_clusters': 2}),\n","    (varied,      {}),\n","    (aniso,       {}),\n","    (blobs, {}),\n","    (no_structure, {})]"]},{"cell_type":"markdown","metadata":{"id":"fqHpT8uchTux"},"source":["#### Plot ground truth cluster assignments"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"bNc5nPk6hTuy"},"outputs":[],"source":["fig, axes = plt.subplots(1,6,figsize=(12,2))\n","\n","for i, (dataset, _) in enumerate(generated_datasets):\n","    ax = axes[i]\n","    X, y = dataset\n","\n","    # normalize dataset for easier parameter selection\n","    X = sklearn.preprocessing.StandardScaler().fit_transform(X)\n","    scprep.plot.scatter2d(X, c=y,\n","                          ticks=None, ax=ax,\n","                          xlabel='x0', ylabel='x1',\n","                         legend=False)\n","\n","fig.tight_layout()"]},{"cell_type":"markdown","metadata":{"id":"Fu7umbXIhTuy"},"source":["### Run clustering algorithms and plot results\n","\n","This is a lot of code, so make sure you and your group go through and understand what's going on. Ask for help if you need!"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"FCvFVyzRhTuz"},"outputs":[],"source":["fig, axes = plt.subplots(6,3, figsize=(12, 20))\n","plot_title = True\n","\n","for i_dataset, (dataset, cluster_params) in enumerate(generated_datasets):\n","    # update cluster parameters with dataset-specific values\n","    params = default_base.copy()\n","    params.update(cluster_params)\n","\n","    X, y = dataset\n","\n","    # normalize dataset for easier parameter selection\n","    X = sklearn.preprocessing.StandardScaler().fit_transform(X)\n","\n","\n","    # ============\n","    # Run clustering algorithms\n","    # ============\n","    clusters = []\n","    titles = []\n","    times = []\n","    # KMeans\n","    tic = time.time()\n","    kmeans = sklearn.cluster.KMeans(n_clusters=params['n_clusters'])\n","    clusters.append(kmeans.fit_predict(X))\n","    titles.append('KMeans')\n","    times.append(time.time() - tic)\n","\n","    # Spectral Clustering\n","    tic = time.time()\n","    spectral = sklearn.cluster.SpectralClustering(\n","        n_clusters=params['n_clusters'], eigen_solver='arpack',\n","        affinity=\"nearest_neighbors\")\n","    clusters.append(spectral.fit_predict(X))\n","    titles.append('Spectral')\n","    times.append(time.time() - tic)\n","\n","\n","    # Louvain\n","    tic = time.time()\n","    G = gt.Graph(X)\n","    G_igraph = G.to_igraph()\n","    part = louvain.find_partition(G_igraph, louvain.RBConfigurationVertexPartition,\n","                                  weights=\"weight\", resolution_parameter=0.01)\n","    clusters.append(np.array(part.membership))\n","    titles.append('Louvain')\n","    times.append(time.time() - tic)\n","\n","    # ============\n","    # Plot clustering results for dataset\n","    # ============\n","    row_axes = axes[i_dataset]\n","\n","    for i, ax in enumerate(row_axes.flatten()):\n","        curr_cluster = clusters[i]\n","        if plot_title:\n","            curr_title = '{}'.format(titles[i])\n","        else:\n","            curr_title = None\n","\n","        scprep.plot.scatter2d(X, c=curr_cluster, title=curr_title, ax=ax,\n","                             legend=False, discrete=True)\n","\n","        # Plot time to run algorithm\n","        plt.text(.99, .01, ('%.2fs' % (times[i])).lstrip('0'),\n","                 transform=ax.transAxes, size=15,\n","                 horizontalalignment='right')\n","    plot_title=False\n","fig.tight_layout()"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"vAaRCn9DhTuz"},"outputs":[],"source":["failing_circles, labels = sklearn.datasets.make_circles(\n","    n_samples=n_samples,\n","    # Scale factor between inner and outer circle\n","    factor=.5,\n","    # Gaussian noise added to each point\n","    noise=.1)\n","spectral = sklearn.cluster.SpectralClustering(\n","        n_clusters=2, eigen_solver='arpack',\n","        affinity=\"nearest_neighbors\")\n","clusters = spectral.fit_predict(failing_circles)\n","scprep.plot.scatter2d(failing_circles, c=clusters, legend=False, discrete=True)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"z9gFJqbphTuz"},"outputs":[],"source":["random_state = 170\n","failing_blobs_aniso, y = sklearn.datasets.make_blobs(n_samples=n_samples, random_state=random_state)\n","# Changes how x1, x2 coordinates are shifted\n","transformation = [[0.2, -0.6], [-0.4, 0.8]]\n","failing_blobs_aniso = np.dot(failing_blobs_aniso, transformation)\n","\n","G = gt.Graph(failing_blobs_aniso)\n","G_igraph = G.to_igraph()\n","part = louvain.find_partition(G_igraph, louvain.RBConfigurationVertexPartition,\n","                              weights=\"weight\", resolution_parameter=0.01)\n","clusters = np.array(part.membership)\n","scprep.plot.scatter2d(failing_blobs_aniso, c=clusters, legend=False, discrete=True)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"h5hQJZyJhTu0"},"outputs":[],"source":["random_state = 170\n","failing_blobs, y = sklearn.datasets.make_blobs(n_samples=n_samples, random_state=random_state, cluster_std=4)\n","kmeans = sklearn.cluster.KMeans(3)\n","clusters = kmeans.fit_predict(failing_blobs)\n","scprep.plot.scatter2d(failing_blobs, c=clusters, legend=False, discrete=True)"]},{"cell_type":"markdown","metadata":{"id":"jFkfc0bbhTu0"},"source":["#### Assignment\n","Please discuss and write your answers:\n","\n","In groups, change the following features for one or more of the distributions\n","1. `noise` - the amount of Gaussian noise added to each point\n","2. `n_samples` - the number of points generated\n","3. `factor` / `cluster_std` / `transformation` - some parameters affecting shape\n","\n","Try to identify:\n","1. A set of parameters that makes `SpectralClustering` fail on the circles dataset\n","2. `Louvain` fail on the anisotropically distributed blobs\n","3. `KMeans` fail on the three regular blobs.\n","\n","\n","After adjusting the Gaussian noise, it becomes evident that increasing the noise amplifies the spread of the point distribution, implying a broader distribution range.\n","\n","Modifying the number of points generated not only slightly expands the distribution range but also intensifies the point concentration within specific areas.\n","\n","While these parameters have subtle effects, they do lead to modifications in the overall shape, distribution, and density. Clusters may appear less dense, and the algorithm might recognize fewer, yet larger, clusters.\n","\n","Setting the number of clusters to 1 is not logical when there are two circles. This results in an erroneous output. Similarly, by increasing the noise to 5, we observe a dispersed point cloud instead of distinct circles.\n","\n","We might consider increasing the number of neighbors significantly, which would force the algorithm to establish nonexistent connections, leading to incorrect algorithm performance.\n","\n","By ramping up the noise to an extreme level, we can mislead Kmeans, resulting in an inaccurate output."]}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.8.5"},"colab":{"provenance":[{"file_id":"1kAFQbvi4ogITz4lQNA4CMq8fDamL81Kl","timestamp":1696836411509},{"file_id":"https://github.com/KrishnaswamyLab/SingleCellWorkshop/blob/master/exercises/Clustering/notebooks/00_Answers_Clustering_toy_data.ipynb","timestamp":1696403381467}]}},"nbformat":4,"nbformat_minor":0}